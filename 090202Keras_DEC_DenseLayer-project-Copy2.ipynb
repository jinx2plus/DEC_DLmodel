{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEC 알고리즘을 대전시 자료를 활용할 수 있도록 수정한 파일\n",
    "입력자료: 44개 검지기, 1일 16시간, 10분간격, 5주간 입력 자료, 교통량과 속도자료 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width: 80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "#import tensorflow.keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import metrics\n",
    "import warnings\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    input_img = Input(shape=(dims[0],), name='input')\n",
    "    x = input_img\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n",
    "    \n",
    "    # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 입력자료를 확인하고 input format 수정\n",
    "\n",
    "trains = np.loadtxt(\"C:/Users/KOTI/Documents/2020 박용진/1. 자료분석/계백로/input_normalize.csv\",\n",
    "                    skiprows=1, delimiter=',', dtype=float)\n",
    "\n",
    "x = trains[:, 267:531].reshape([len(trains), 264])  # 열의 갯수를 수정\n",
    "y_year = trains[:, 0].reshape([len(trains)])\n",
    "y_weekday = trains[:, 1].reshape([len(trains)])\n",
    "y = trains[:, 2].reshape([len(trains)])\n",
    "\n",
    "x = x.reshape((x.shape[0], -1))\n",
    "#x = np.divide(x, 256.)        #normalized 데이터에서는 삭제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([20190818., 20190818., 20190818., ..., 20190921., 20190921.,\n",
      "       20190921.]), array([1., 1., 1., ..., 7., 7., 7.]), array([ 600.,  610.,  620., ..., 2130., 2140., 2150.])]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 5 #len(np.unique(y))  # cluster의 갯수를 직접 선언해야 함\n",
    "x.shape\n",
    "print ([y_year, y_weekday, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20, n_jobs=4)\n",
    "y_pred_kmeans = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'metrics' has no attribute 'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-16cc57220acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_kmeans\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 우리 자료에는 label이 없으므로 accuracy를 구할 수 없음\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'metrics' has no attribute 'acc'"
     ]
    }
   ],
   "source": [
    "metrics.acc(y, y_pred_kmeans)  # 우리 자료에는 label이 없으므로 accuracy를 구할 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dims = [x.shape[-1], 500, 500, 2000, 10]\n",
    "dims = [x.shape[-1], 500, 1000, 2000, 3000, 5000, 10]   # hidden layer를 어떻게 구성할 것인지 고민 \n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                           distribution='uniform')\n",
    "pretrain_optimizer = SGD(lr=0.1, momentum=0.9)\n",
    "pretrain_epochs = 1000\n",
    "batch_size = 256\n",
    "save_dir = './results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:699: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:601: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:865: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2290: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:332: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:340: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "3360/3360 [==============================] - 7s - loss: 0.1005     \n",
      "Epoch 2/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0855     \n",
      "Epoch 3/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0680     \n",
      "Epoch 4/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0512     \n",
      "Epoch 5/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0356     \n",
      "Epoch 6/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0223     \n",
      "Epoch 7/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0136     \n",
      "Epoch 8/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0107     \n",
      "Epoch 9/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0102     \n",
      "Epoch 10/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0101     \n",
      "Epoch 11/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0101     \n",
      "Epoch 12/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0101     \n",
      "Epoch 13/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0101     \n",
      "Epoch 14/1000\n",
      "3360/3360 [==============================] - 6s - loss: 0.0100     \n",
      "Epoch 15/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0100     \n",
      "Epoch 16/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0100     \n",
      "Epoch 17/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0100     \n",
      "Epoch 18/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0100     \n",
      "Epoch 19/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0100     \n",
      "Epoch 20/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0099     \n",
      "Epoch 21/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0099     \n",
      "Epoch 22/1000\n",
      "3360/3360 [==============================] - 6s - loss: 0.0099     \n",
      "Epoch 23/1000\n",
      "3360/3360 [==============================] - 5s - loss: 0.0098     \n",
      "Epoch 24/1000\n",
      "3072/3360 [==========================>...] - ETA: 0s - loss: 0.0098"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs) #, callbacks=cb)\n",
    "autoencoder.save_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape = (self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name ='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))\n",
    "y_pred_last = np.copy(y_pred)\n",
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "# computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n",
    "\n",
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 20000\n",
    "update_interval = 140\n",
    "index_array = np.arange(x.shape[0])\n",
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.04613, nmi = 0.35613, ari = 0.02960  ; loss= 0\n",
      "Iter 140: acc = 0.04643, nmi = 0.34686, ari = 0.03027  ; loss= 0.0259\n",
      "Iter 280: acc = 0.04673, nmi = 0.33765, ari = 0.03041  ; loss= 0.03754\n",
      "Iter 420: acc = 0.04524, nmi = 0.33077, ari = 0.02963  ; loss= 0.05804\n",
      "Iter 560: acc = 0.04405, nmi = 0.32464, ari = 0.02894  ; loss= 0.07754\n",
      "Iter 700: acc = 0.04345, nmi = 0.32005, ari = 0.02843  ; loss= 0.09309\n",
      "Iter 840: acc = 0.04226, nmi = 0.31808, ari = 0.02819  ; loss= 0.1031\n",
      "Iter 980: acc = 0.04286, nmi = 0.31423, ari = 0.02727  ; loss= 0.11036\n",
      "Iter 1120: acc = 0.04196, nmi = 0.32956, ari = 0.02944  ; loss= 0.10942\n",
      "Iter 1260: acc = 0.02292, nmi = 0.14111, ari = 0.00105  ; loss= 1.16853\n",
      "Iter 1400: acc = 0.02411, nmi = 0.14359, ari = 0.00105  ; loss= 0.01224\n",
      "Iter 1540: acc = 0.02440, nmi = 0.14432, ari = 0.00107  ; loss= 0.01323\n",
      "Iter 1680: acc = 0.02411, nmi = 0.14440, ari = 0.00107  ; loss= 0.01487\n",
      "Iter 1820: acc = 0.02381, nmi = 0.14425, ari = 0.00107  ; loss= 0.0146\n",
      "Iter 1960: acc = 0.02351, nmi = 0.14407, ari = 0.00108  ; loss= 0.0198\n",
      "delta_label  0.0005952380952380953 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "            nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "            ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion - model convergence\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc = 0.04048, nmi = 0.26889, ari = 0.02213  ; loss= 0.03568\n"
     ]
    }
   ],
   "source": [
    "# Eval.\n",
    "############ redefine test data set ##############\n",
    "\n",
    "test = np.loadtxt(\"C:/Users/KOTI/Documents/2020 박용진/1. 자료분석/계백로/input_raw.csv\", skiprows=1, delimiter=',', dtype=float)\n",
    "x = test[:, 267:531].reshape([len(test), 264])\n",
    "\n",
    "y_year = test[:, 0].reshape([len(test)])\n",
    "y_weekday = test[:, 1].reshape([len(test)])\n",
    "y = test[:, 2].reshape([len(test)])\n",
    "\n",
    "x = x.reshape((x.shape[0], -1))\n",
    "#x = np.divide(x, 255.)    #normalized 데이터에서는 삭제\n",
    "\n",
    "\n",
    "q = model.predict(x, verbose=0)\n",
    "p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q.argmax(1)\n",
    "if y is not None:\n",
    "    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "    loss = np.round(loss, 5)\n",
    "    print('Acc = %.5f, nmi = %.5f, ari = %.5f' % (acc, nmi, ari), ' ; loss=', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=3)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=30)\n",
    "plt.ylabel('True label', fontsize=25)\n",
    "plt.xlabel('Clustering label', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04047619047619048"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "\n",
    "y_true = y.astype(np.int64)\n",
    "D = max(y_pred.max(), y_true.max()) + 1\n",
    "w = np.zeros((D, D), dtype=np.int64)\n",
    "# Confusion matrix.\n",
    "for i in range(y_pred.size):\n",
    "    w[y_pred[i], y_true[i]] += 1\n",
    "ind = linear_assignment(-w)\n",
    "\n",
    "sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "df = pd.DataFrame(y_year, columns = ['year'])\n",
    "df['weekday'] = y_weekday\n",
    "df['true'] = y\n",
    "df['cluster_id'] = y_pred\n",
    "df.to_excel('C:/Users/KOTI/project/AI_traffic_management/Deep Clustering/DenseLayer/results/test.xlsx', index = False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
